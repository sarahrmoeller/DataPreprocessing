{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code by Zachary J. Ryan \n",
    "at CU Boulder\n",
    "Spring 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API for parsing XML docs\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import chain\n",
    "import sklearn\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "def XMLtoArray(filename, stems=False):\n",
    "    '''Takes multiple FLExText texts as .xml. \n",
    "    Returns data as list: [[[[morpheme, gloss, mpos, wpos],...]word,...]sent,...]'''\n",
    "    \n",
    "    # Identify tiers, because the info you need may be on different tiers in FLEx.\n",
    "    txt = 'txt'\n",
    "    gloss = 'gls'\n",
    "    cf = 'cf'\n",
    "    pos = 'pos' # word-level pos\n",
    "    msa = 'msa' # morpheme-level pos\n",
    "    punct = 'punct'\n",
    "    title_type = 'title'\n",
    "    comment_type = 'comment'\n",
    "    english = 'en'\n",
    "    \n",
    "    datalists = []\n",
    "    #a single line for datalists is below\n",
    "    #datalists -> [title, segnum, [[word_1,[[morph, gloss]], pos], [word2,[[morph, gloss], [morph, gloss]], pos], ... ,[wordn,[[morph, gloss], [morph, gloss]], pos]], translated phrase, comment]\n",
    "    \n",
    "    # open XML doc using xml parser\n",
    "    root = ET.parse(filename).getroot()\n",
    "    for lin in root.iter('interlinear-text'):\n",
    "        #find title and comment if in this section\n",
    "        #some documents have both and english and native language titles\n",
    "        #these checks assure that the english title will always be used if both are found\n",
    "        #if only one of them is found then it is used\n",
    "        #if none are found return NO TITLE FOUND\n",
    "        comment = \"No comment\"\n",
    "        eng_title = \"~~~\"\n",
    "        non_eng_title = \"~~~\"\n",
    "        for item_lin in lin.iter('item'):\n",
    "            if item_lin.get('type') == title_type and item_lin.get('lang') == english:\n",
    "                eng_title = item_lin.text\n",
    "            if item_lin.get('type') == title_type and item_lin.get('lang') != english:\n",
    "                non_eng_title = item_lin.text\n",
    "            if item_lin.get('type') == comment_type and item_lin.get('lang') == english:\n",
    "                comment = item_lin.text\n",
    "        if eng_title != \"~~~\" and non_eng_title == \"~~~\":\n",
    "            title = eng_title\n",
    "        elif eng_title == \"~~~\" and non_eng_title != \"~~~\":\n",
    "            title = non_eng_title\n",
    "        elif eng_title != \"~~~\" and non_eng_title != \"~~~\":\n",
    "            title = eng_title\n",
    "        else:\n",
    "            title = \"NO TITLE FOUND\"\n",
    "            \n",
    "        #go through all paragraphs\n",
    "        for paragraphs in lin.iter('paragraphs'):\n",
    "            for paragraph in paragraphs.iter('paragraph'):\n",
    "                #go through all phrases in paragraph\n",
    "                for phrases in paragraph.iter('phrases'):\n",
    "                    for phrase in phrases.iter('phrase'):\n",
    "                        #create a temp array for each phrase\n",
    "                        temp_line = []\n",
    "                        #append title\n",
    "                        temp_line.append(title)\n",
    "                        #get segnum and append to temp_line\n",
    "                        segnum = phrase.find('item').text\n",
    "                        temp_line.append(segnum)\n",
    "                        temp_words_morph_gloss = []\n",
    "                        #loop through all words in the phrase\n",
    "                        for words in phrase.iter('words'):\n",
    "                            for word in words.iter('word'):\n",
    "                                temp_word = []\n",
    "                                #attach the untranslated word\n",
    "                                wrd = word.find('item')\n",
    "                                if wrd.get('type') == punct:\n",
    "                                    temp_word.append(wrd.text)\n",
    "                                    # make sure all new entries go with in both sets of brackets, \n",
    "                                    #follow the same format right after 'punct'\n",
    "                                    temp_morph = [[str(wrd.text), str(wrd.text), 'punct']]\n",
    "                                    temp_word.append(temp_morph)\n",
    "                                    temp_word.append('punct')\n",
    "                                    \n",
    "                                else:\n",
    "                                    temp_word.append(wrd.text.replace(' ', '~'))\n",
    "                                    #find the morpheme and gloss for each word and append what is found in the xml\n",
    "                                    temp_morphemes = []\n",
    "                                    for morph in word.iter('morph'):\n",
    "                                        #if you want to add more items that are found in each morpheme add the code here\n",
    "                                        #first add another holding place in the the temp_morph array\n",
    "                                        #then add an elif statement that is of the same type already seen but instead \n",
    "                                        #check for whatever tag you want, here 'gloss' is one.\n",
    "                                        #also if you do add more entries into the temp_morph you should keep it uniform\n",
    "                                        #for the punctuation option above and entries to the temp morph as well.\n",
    "                                        temp_morph = [\"~~~\", \"~~~\", \"~~~\"]\n",
    "                                        for item in morph.iter('item'):\n",
    "                                            if(item.get('type') == cf):\n",
    "                                                temp_morph[0] = item.text\n",
    "                                            elif(item.get('type') == gloss):\n",
    "                                                # separate multi-word glosses with \".\"\n",
    "                                                gloss_line = item.text\n",
    "                                                if gloss_line != None:\n",
    "                                                    gloss_line = gloss_line.strip().replace(' ','.').replace('-','.')\n",
    "                                                    temp_morph[1] = gloss_line\n",
    "                                            elif(item.get('type') == msa):\n",
    "                                                temp_morph[2] = item.text.replace(' ', '').replace('pro-form', 'proform')\n",
    "                                            else:\n",
    "                                                continue\n",
    "                                        temp_morphemes.append(temp_morph)\n",
    "                                    temp_word.append(temp_morphemes)\n",
    "                                    #look for pos for the individual word\n",
    "                                    temp_pos = '~~~'\n",
    "                                    for w_item in word.iter('item'):\n",
    "                                        if(w_item.get('type') == pos):\n",
    "                                            temp_pos = w_item.text.replace('pro-form', 'proform')\n",
    "                                        else:\n",
    "                                            continue\n",
    "                                    temp_word.append(temp_pos)\n",
    "                                temp_words_morph_gloss.append(temp_word)\n",
    "                            #finding the phrase translation, starts with a string of '~~~'\n",
    "                            translation = '~~~'\n",
    "                            #iterate through all 'item' in branch phrase\n",
    "                            temp_phrase_gloss = [p_item for p_item in phrase.iter('item')]\n",
    "                            #take the last item which should be our phrase translation but the if statement checks to make sure\n",
    "                            for tmg in temp_phrase_gloss:\n",
    "                                if tmg.get('type') == gloss and tmg.get('lang') == english:\n",
    "                                    translation = tmg.text\n",
    "                            #append all the words with there morpheme and gloss array\n",
    "                            temp_line.append(temp_words_morph_gloss)\n",
    "                            #append the translation of the phrase to the end of the temp line\n",
    "                            temp_line.append(translation)\n",
    "                            #append whatever comment may have been found\n",
    "                            temp_line.append(comment)\n",
    "                            #append each phrase to datalist\n",
    "                            #print(temp_line)\n",
    "                            datalists.append(temp_line)\n",
    "    \n",
    "    return datalists\n",
    "\n",
    "\n",
    "def arrayToCSV(xml, langID, fileName):\n",
    "    \n",
    "    with open(fileName, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        for i in range(0,len(xml)):\n",
    "            #starts with segnum and lang ID\n",
    "            csv_line = [xml[i][1],langID]\n",
    "            temp_line_phrase = \"\"\n",
    "            temp_line_morph = \"\"\n",
    "            temp_line_gloss = \"\"\n",
    "            temp_line_msa = \"\"\n",
    "            temp_line_word_pos = \"\"\n",
    "            for wmg in range(0,len(xml[i][2])):\n",
    "                temp_line_phrase += (str(xml[i][2][wmg][0])+\" \")\n",
    "                temp_morph = \"\"\n",
    "                temp_gloss = \"\"\n",
    "                temp_msa = \"\"\n",
    "                temp_line_word_pos += (str(xml[i][2][wmg][2])+\" \")\n",
    "                for mg in range(0, len(xml[i][2][wmg][1])):\n",
    "                    if mg == (len(xml[i][2][wmg][1])-1):\n",
    "                        temp_morph += (str(xml[i][2][wmg][1][mg][0]))\n",
    "                        temp_gloss += (str(xml[i][2][wmg][1][mg][1]))\n",
    "                        temp_msa   += (str(xml[i][2][wmg][1][mg][2]))\n",
    "                    else:\n",
    "                        temp_morph += (str(xml[i][2][wmg][1][mg][0])+\" \")\n",
    "                        temp_gloss += (str(xml[i][2][wmg][1][mg][1])+\"-\")\n",
    "                        temp_msa   += (str(xml[i][2][wmg][1][mg][2])+\"-\")\n",
    "                temp_line_morph += (temp_morph+\"\\t\")\n",
    "                temp_line_gloss += (temp_gloss+\"\\t\")\n",
    "                temp_line_msa += (temp_msa+\"\\t\")\n",
    "                \n",
    "                \n",
    "            csv_line.append(temp_line_phrase)\n",
    "            csv_line.append(temp_line_morph)\n",
    "            csv_line.append(temp_line_gloss)\n",
    "            csv_line.append(xml[i][3]) #appends the english translation\n",
    "            csv_line.append(\"ML ID\")\n",
    "            csv_line.append(xml[i][4]) #append the comment found from file\n",
    "            csv_line.append(xml[i][0]) #appends the title or Text_ID\n",
    "            csv_line.append(temp_line_msa) #appends the morphemes POS\n",
    "            csv_line.append(temp_line_word_pos) #appends the POS for each word \n",
    "            \n",
    "            writer.writerow(csv_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Right Attitude, the Right Form', '1', [['khúdə́m', [['khút', 'hand', 'n'], ['lə́m', 'path', 'n']], 'n'], ['oynə', [['oy', 'be', '<NotSure>'], ['-nə', '.ADV', 'Attachestoanycategory']], '~~~'], [',', [[',', ',', 'punct']], 'punct'], ['yam', [['yam', 'much', '<NotSure>']], '~~~'], ['waŋnə', [['waŋ', 'high', '<NotSure>'], ['-nə', '.ADV', 'Attachestoanycategory']], '~~~'], ['haygətnə', [['hay', 'sway', 'v'], ['-khət', '.UP', 'v>v'], ['-nə', '.ADV', 'Attachestoanycategory']], 'v'], ['kəyno', [['kəri', 'what', '<NotSure>'], ['=no', '=INQ', '<NotSure>']], 'adv'], ['təwbidrə́gəsú', [['təw', 'do', '<NotSure>'], ['-pi', '.REC', 'Attachestoanycategory'], ['-tə', '.NEG', '<NotSure>'], ['-lə́gə', '.AFTER', 'Attachestoanycategory'], ['-čhú', '.ALSO', 'Attachestoanycategory']], '~~~'], [',', [[',', ',', 'punct']], 'punct'], ['tə́rahumdə', [['tə́ra', 'ten', '<NotSure>'], ['hum', 'three', '<NotSure>'], ['=tə', '==LOC', '<NotSure>']], '~~~'], ['oybəsú', [['oy', 'be', '<NotSure>'], ['-pə', '.NOM', 'v>n'], ['-čhú', '.ALSO', 'Attachestoanycategory']], '~~~'], ['parti', [['parti', 'party', 'n']], 'n'], [',', [[',', ',', 'punct']], 'punct'], ['koŋgrés', [['koŋgrés', 'Congress', 'n']], 'n'], ['ŋə́sidəgi', [['ŋə́čhi', 'today', '<NotSure>'], ['=təgi', '==ABL', '<NotSure>']], '~~~'], ['čə́hí', [['čə́hí', 'year', 'n']], 'n'], ['mə́ŋaromgi', [['mə́ŋa', 'five', '<NotSure>'], ['-lom', '.APX', 'Attachestoanycategory'], ['=ki', '==POSS', '<NotSure>']], '~~~'], ['mə́maŋdə', [['mə́-', 'NM.', 'Attachestoanycategory'], ['maŋ', 'front', '<NotSure>'], ['=tə', '==LOC', '<NotSure>']], '~~~'], ['phə́mkhibə', [['phə́m', 'sit', 'v'], ['-khi', '.STILL', 'Attachestoanycategory'], ['-pə', '.NOM', 'v>n']], '~~~'], [',', [[',', ',', 'punct']], 'punct'], ['mə́dúgi', [['mə́-', 'NM.', 'Attachestoanycategory'], ['tú', 'ddet', '<NotSure>'], ['=ki', '==POSS', '<NotSure>']], '~~~'], ['mə́thə́k', [['mə́-', 'NM.', 'Attachestoanycategory'], ['thə́k', 'up', '<NotSure>']], '~~~'], ['čə́hí', [['čə́hí', 'year', 'n']], 'n'], ['tə́ramúkki', [['tə́ra', 'ten', '<NotSure>'], ['múk', 'once', '<NotSure>'], ['=ki', '==POSS', '<NotSure>']], '~~~'], ['mə́maŋdə', [['mə́-', 'NM.', 'Attachestoanycategory'], ['maŋ', 'front', '<NotSure>'], ['=tə', '==LOC', '<NotSure>']], '~~~'], ['phə́mkhibə', [['phə́m', 'sit', 'v'], ['-khi', '.STILL', 'Attachestoanycategory'], ['-pə', '.NOM', 'v>n']], '~~~'], [',', [[',', ',', 'punct']], 'punct'], ['kəya', [['kəya', 'how.many', '<NotSure>']], '~~~'], ['ə́mə', [['ə́-', 'ATT.', 'Attachestoanycategory'], ['mə', 'one', '<NotSure>']], '~~~'], ['píbə', [['pí', 'give', 'v'], ['-pə', '.NOM', 'v>n']], '~~~'], ['yay', [['ya', 'possible', '<NotSure>'], ['-í', '.NHYP', 'Verb']], 'n'], ['.', [['.', '.', 'punct']], 'punct']], \"...for example, even if you don't turn back the pages further than fifteen to twenty years ago, you can think of appropriate examples. That thirteenth Congress that was in power five years ago, and also the ones that were in power ten years previous to those five years, these can be given as examples.\", 'No comment']\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DIR = r'../'\n",
    "#lgs = ['btz', 'lez', 'lez', 'ntu', 'mni']\n",
    "lgs = ['mni']\n",
    "for lg in lgs:\n",
    "    source_file = SOURCE_DIR + lg + '-all_txts.flextext'\n",
    "    datalists = XMLtoArray(source_file, stems=True)\n",
    "    print(datalists[0])\n",
    "    outputfile = lg + '-CLDF.csv'\n",
    "    arrayToCSV(datalists, lg, outputfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
